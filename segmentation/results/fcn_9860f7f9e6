# FROM TRAINING CURVES
## Lambda
<0.1 look alright.
0.1 causes more noise in the latest epochs (seem like abit of overfitting at start)
10 straight up diverges converging to a worst minima (~0.7).

## Learning rate 
All converge fine.
<=0.001 converges to a bad value. In the val loss curve, loss goes initally up and then decreases (diff to fcn wo batchnorm)
0.01 look alright.
0.1 looks alright, too although in val losses the differences between diff runs is more noticeable, converges to better solution, though.

###############################################################################
# VALIDATION SET
## Average best val loss (less is better)
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0	    	| 0.645	| 0.251	| 0.203	| 0.156	| 1.696	|
| 1e-05		| 0.644	| 0.25	| 0.204	| 0.157	| 1.698	|
| 0.001		| 0.645	| 0.251	| 0.202	| 0.15	| 1.346	|
| 0.1		| 0.636	| 0.265	| 0.193	| 0.221	| 0.278	|
| 10		| 0.656	| 0.652	| 0.653	| 0.656	| inf	|

## Average best epoch
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0	    	| 95.0	| 99.0	| 76.0	| 85.0	| 13.0	|
| 1e-05		| 96.0	| 96.0	| 71.0	| 82.0	| 12.0	|
| 0.001		| 96.0	| 98.0	| 73.0	| 84.0	| 12.0	|
| 0.1		| 95.0	| 94.0	| 94.0	| 78.0	| 3.0	|
| 10		| 97.0	| 2.0	| 1.0	| 49.0	| 0.0	|

# Average best threshold
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.15	| 0.1	| 0.14	| 0.25	| 0.05	|
| 1e-05		| 0.15	| 0.1	| 0.13	| 0.24	| 0.05	|
| 0.001		| 0.16	| 0.1	| 0.14	| 0.27	| 0.05	|
| 0.1		| 0.15	| 0.14	| 0.22	| 0.15	| 0.0	|
| 10		| 0.0	| 0.0	| 0.0	| 0.0	| 0.0	|

# Average best IOU
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0	    	| 0.067	| 0.099	| 0.224	| 0.403	| 0.069	|
| 1e-05		| 0.067	| 0.1	| 0.222	| 0.4	| 0.057	|
| 0.001		| 0.067	| 0.099	| 0.23	| 0.412	| 0.075	|
| 0.1		| 0.067	| 0.103	| 0.405	| 0.284	| 0.066	|
| 10		| 0.066	| 0.066	| 0.066	| 0.066	| 0.066	|

Conclusion:
0.1 lr
1e-4 lambda (between 1e-5 and 1e-3)


###############################################################################
# Training set

# Average best IOU
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.067	| 0.099	| 0.204	| 0.388	| 0.061	|
| 1e-05		| 0.067	| 0.099	| 0.202	| 0.385	| 0.052	|
| 0.001		| 0.067	| 0.099	| 0.208	| 0.398	| 0.056	|
| 0.1		| 0.067	| 0.1	| 0.389	| 0.285	| 0.065	|
| 10		| 0.065	| 0.065	| 0.065	| 0.065	| 0.065	|


################################################################################
# No learning rate schedule
################################################################################
# VAL results
For lr 0.01 and 0.01, 3 out of the 8 val loss functions seem to have higher losses.
For lr 0.1, the val loss even decreases initially goes up again and decreases again. It seems a bit more noisy , too
I could use 0.1 which gives best and then decrease lr for the second 100 epochs (so it converges finally into local maxima)
Higher lambdas cause divergence.


## Average best val loss (less is better)
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.38	| 0.247	| 0.193	| 0.132	| 1.715	|
| 1e-05		| 0.379	| 0.247	| 0.193	| 0.131	| 1.719	|
| 0.001		| 0.38	| 0.247	| 0.19	| 0.127	| 1.366	|
| 0.1		| 0.387	| 0.259	| 0.189	| 0.257	| 0.278	|
| 10		| 0.656	| 0.652	| 0.653	| 0.656	| inf |


## Average best epoch
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0	    	| 99.0	| 92.0	| 76.0	| 96.0	| 1.0	|
| 1e-05		| 99.0	| 92.0	| 76.0	| 98.0	| 1.0	|
| 0.001		| 99.0	| 92.0	| 87.0	| 90.0	| 1.0	|
| 0.1		| 99.0	| 100.0	| 80.0	| 13.0	| 14.0	|
| 10		| 82.0	| 2.0	| 1.0	| 36.0	| 0.0	|

# Average best threshold
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.22	| 0.09	| 0.14	| 0.36	| 0.0	|
| 1e-05		| 0.22	| 0.09	| 0.14	| 0.34	| 0.0	|
| 0.001		| 0.22	| 0.09	| 0.14	| 0.29	| 0.0	|
| 0.1		| 0.25	| 0.14	| 0.31	| 0.0	| 0.0	|
| 10		| 0.0	| 0.0	| 0.0	| 0.0	| 0.0	|

# Average best IOU
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.07	| 0.111	| 0.291	| 0.446	| 0.026	|
| 1e-05		| 0.07	| 0.111	| 0.291	| 0.443	| 0.009	|
| 0.001		| 0.07	| 0.111	| 0.301	| 0.454	| 0.017	|
| 0.1		| 0.07	| 0.124	| 0.37	| 0.066	| 0.066	|
| 10		| 0.066	| 0.066	| 0.066	| 0.066	| 0.066	|


###############################################################################
# Weighted loss + no learning rate schedule
###############################################################################
# Val results
Slow lrs do not learn, 0.01 and 0.1 is more noisy (still acceptable) but converge to better results
lambda of 0.1, 10 cause no learning or divergence (too high), 0.1. 0.001 seems to be sweet point
Even with learning rate 1, this does not diverge

## Average best val loss (less is better)
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.581	| 0.485	| 0.325	| 0.233	| 1.626	|
| 1e-05		| 0.582	| 0.486	| 0.322	| 0.233	| 1.723	|
| 0.001		| 0.582	| 0.485	| 0.316	| 0.227	| 0.271	|
| 0.1		| 0.576	| 0.471	| 0.309	| 0.527	| 0.548	|
| 10		| 0.683	| 0.682	| 0.682	| 0.681	| inf	|

## Average best epoch
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 99.0	| 78.0	| 99.0	| 94.0	| 53.0	|
| 1e-05		| 99.0	| 79.0	| 98.0	| 92.0	| 56.0	|
| 0.001		| 99.0	| 79.0	| 99.0	| 90.0	| 80.0	|
| 0.1		| 99.0	| 82.0	| 68.0	| 19.0	| 55.0	|
| 10		| 37.0	| 2.0	| 47.0	| 74.0	| 0.0	|

## Average best threshold
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.38	| 0.24	| 0.51	| 0.7	| 0.41	|
| 1e-05		| 0.39	| 0.25	| 0.51	| 0.66	| 0.41	|
| 0.001		| 0.39	| 0.25	| 0.51	| 0.66	| 0.43	|
| 0.1		| 0.39	| 0.28	| 0.52	| 0.0	| 0.0	|
| 10		| 0.0	| 0.0	| 0.0	| 0.0	| 0.0	|

## Average best IOU
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.08	| 0.148	| 0.383	| 0.46	| 0.274	|
| 1e-05		| 0.08	| 0.148	| 0.385	| 0.461	| 0.275	|
| 0.001		| 0.08	| 0.148	| 0.388	| 0.467	| 0.359	|
| 0.1		| 0.08	| 0.171	| 0.368	| 0.066	| 0.066	|
| 10		| 0.066	| 0.066	| 0.066	| 0.066	| 0.066	|


##############################################################################
# Weighted loss + learning rate schedule
##############################################################################
# Val Losses
All lrs seem to learn alright, 0.01 and 0.1 is more noisy (still acceptable) but converge to better results
lambda of 10 cause no learning or divergence (too high), 0.1, does sometimes, too

## Average best val loss (less is better)
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.713	| 0.5	| 0.379	| 0.283	| 0.394	|
| 1e-05		| 0.711	| 0.499	| 0.379	| 0.279	| 0.396	|
| 0.001		| 0.711	| 0.499	| 0.377	| 0.275	| 0.29	|
| 0.1		| 0.71	| 0.489	| 0.302	| 0.343	| 0.549	|
| 10		| 0.683	| 0.682	| 0.682	| 0.682	| inf	|

## Average best epoch
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 24.0	| 79.0	| 70.0	| 83.0	| 81.0	|
| 1e-05		| 24.0	| 78.0	| 69.0	| 74.0	| 80.0	|
| 0.001		| 25.0	| 78.0	| 67.0	| 76.0	| 79.0	|
| 0.1		| 25.0	| 80.0	| 90.0	| 96.0	| 7.0	|
| 10		| 59.0	| 2.0	| 3.0	| 22.0	| 0.0	|

## Average best threshold
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.21	| 0.27	| 0.38	| 0.61	| 0.44	|
| 1e-05		| 0.21	| 0.27	| 0.39	| 0.61	| 0.38	|
| 0.001		| 0.21	| 0.27	| 0.39	| 0.61	| 0.6	|
| 0.1		| 0.22	| 0.29	| 0.54	| 0.41	| 0.0	|
| 10		| 0.06	| 0.0	| 0.0	| 0.0	| 0.0	|

## Average best IOU
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.069	| 0.131	| 0.288	| 0.428	| 0.252	|
| 1e-05		| 0.069	| 0.131	| 0.288	| 0.428	| 0.256	|
| 0.001		| 0.069	| 0.132	| 0.291	| 0.435	| 0.417	|
| 0.1		| 0.069	| 0.142	| 0.43	| 0.355	| 0.066	|
| 10		| 0.066	| 0.066	| 0.066	| 0.066	| 0.066	|



##############################################################################
Weighted loss + no learning rate schedule + input preprocessing
##############################################################################
Learning curves look pretty ok and as if they may still go down a little bit for lr >= 0.01
0.1 is a bit noisy but not as bad as with simple input. 1 still seems to converge fine
High lambdas are again counterproductive

## Average best val loss (less is better)
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.646	| 0.444	| 0.285	| 0.217	| 0.204	|
| 1e-05		| 0.646	| 0.444	| 0.285	| 0.217	| 0.204	|
| 0.001		| 0.645	| 0.443	| 0.282	| 0.21	| 0.216	|
| 0.1		| 0.634	| 0.434	| 0.265	| 0.405	| 0.547	|
| 10		| 0.681	| 0.676	| 0.679	| 0.677	| inf	|

## Average best epoch
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 87.0	| 100.0	| 99.0	| 93.0	| 93.0	|
| 1e-05		| 87.0	| 100.0	| 99.0	| 93.0	| 94.0	|
| 0.001		| 87.0	| 100.0	| 99.0	| 94.0	| 67.0	|
| 0.1		| 100.0	| 100.0	| 91.0	| 38.0	| 24.0	|
| 10		| 98.0	| 2.0	| 1.0	| 58.0	| 0.0	|

## Average best threshold
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.47	| 0.45	| 0.6	| 0.68	| 0.77	|
| 1e-05		| 0.47	| 0.45	| 0.6	| 0.68	| 0.78	|
| 0.001		| 0.47	| 0.45	| 0.6	| 0.72	| 0.59	|
| 0.1		| 0.47	| 0.42	| 0.68	| 0.34	| 0.0	|
| 10		| 0.0	| 0.0	| 0.0	| 0.0	| 0.0	|

## Average best IOU
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.077	| 0.201	| 0.421	| 0.493	| 0.509	|
| 1e-05		| 0.077	| 0.201	| 0.421	| 0.493	| 0.508	|
| 0.001		| 0.077	| 0.201	| 0.425	| 0.5	| 0.469	|
| 0.1		| 0.078	| 0.23	| 0.473	| 0.123	| 0.066	|
| 10		| 0.066	| 0.066	| 0.066	| 0.066	| 0.066	|

Conclusion:
Definitely better than simple inputs: 0.47->0.51.
It could be overfitting a bit.

