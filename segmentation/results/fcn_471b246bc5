# FROM TRAINING CURVES
## Lambda
All less than 10 look alright. 10 straight up diverges converging to a worst minima.

## Learning rate 
All converge fine.
<=0.001 gives a more gradual decline
0.01 is faster and seems to converge alright but to a suboptimal value
0.1 is a bit erratic: slightly overshoots at first then converges but has more
    variability for diff val examples and during the same example. Not extremely
    high, though

Conclusion: Maybe learning rate decrease is too much and that's why learning 
    slows down: worth trying no lr schedule.
    More epochs could definitely help too.

## Average best val loss (less is better)
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1     |
| 0		    | 0.302	| 0.26	| 0.187	| 0.173	| inf	|
| 1e-05		| 0.302	| 0.26	| 0.188	| 0.171	| inf	|
| 0.001		| 0.302	| 0.26	| 0.187	| 0.185	| inf	|
| 0.1		| 0.301	| 0.272	| 0.26	| 0.26	| 14.524	|
| 10		| 0.527	| 0.607	| 0.652	| 0.656	| inf	|

## Average best epoch
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1	|
| 0		    | 99.0	| 98.0	| 91.0	| 90.0	| 24.0	|
| 1e-05		| 100.0	| 97.0	| 96.0	| 86.0	| 25.0	|
| 0.001		| 100.0	| 96.0	| 92.0	| 94.0	| 26.0	|
| 0.1		| 100.0	| 96.0	| 58.0	| 2.0	| 1.0	|
| 10		| 1.0	| 1.0	| 1.0	| 44.0	| 0.0	|

# Average best threshold
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.03	| 0.12	| 0.17	| 0.19	| 0.0	|
| 1e-05		| 0.03	| 0.12	| 0.16	| 0.2	| 0.0	|
| 0.001		| 0.03	| 0.11	| 0.17	| 0.16	| 0.0	|
| 0.1		| 0.04	| 0.05	| 0.06	| 0.0	| 0.0	|
| 10		| 0.0	| 0.0	| 0.0	| 0.0	| 0.0	|

# Average best IOU
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.068	| 0.092	| 0.276	| 0.335	| 0.057	|
| 1e-05		| 0.068	| 0.091	| 0.273	| 0.339	| 0.057	|
| 0.001		| 0.068	| 0.092	| 0.275	| 0.299	| 0.057	|
| 0.1		| 0.068	| 0.069	| 0.071	| 0.066	| 0.0	|
| 10		| 0.066	| 0.066	| 0.066	| 0.066	| 0.0	|


Conclusion: 
0.1 learning rate seems to be doing better (even though learning curves seemed a
    bit more erratic)
Smaller lambdas (or zero) do better


###############################################################################
# TRAINING SET

# Average best IOU
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.067	| 0.087	| 0.265	| 0.323	| 0.057	|
| 1e-05		| 0.067	| 0.086	| 0.262	| 0.326	| 0.057	|
| 0.001		| 0.067	| 0.088	| 0.265	| 0.293	| 0.057	|
| 0.1		| 0.067	| 0.068	| 0.071	| 0.065	| 0.0	|
| 10		| 0.065	| 0.065	| 0.065	| 0.065	| 0.0	|

Consistently lower than IOU in val (may be because val was used for early stopping).
Trends are consistently the same, though



################################################################################
# No learning rate schedule
################################################################################
# Val results
Training and val losses look ok.
For 0.1 (as with batchnorm fcn_986..), 3 out of 4 val_loss curves converge to a worst loss.

## Average best val loss (less is better)
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.301	| 0.246	| 0.169	| 0.151	| inf	|
| 1e-05		| 0.301	| 0.246	| 0.169	| 0.155	| inf	|
| 0.001		| 0.301	| 0.246	| 0.169	| 0.162	| inf	|
| 0.1		| 0.301	| 0.268	| 0.26	| 0.26	| 14.524	|
| 10		| 0.527	| 0.607	| 0.652	| 0.656	| inf	|

## Average best epoch
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0	    	| 100.0	| 100.0	| 99.0	| 95.0	| 36.0	|
| 1e-05		| 100.0	| 99.0	| 98.0	| 96.0	| 47.0	|
| 0.001		| 100.0	| 100.0	| 97.0	| 83.0	| 50.0	|
| 0.1		| 100.0	| 99.0	| 65.0	| 19.0	| 1.0	|
| 10		| 1.0	| 1.0	| 1.0	| 34.0	| 0.0	|

## Average best threshold
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.04	| 0.13	| 0.2	| 0.27	| 0.0	|
| 1e-05		| 0.04	| 0.14	| 0.19	| 0.27	| 0.0	|
| 0.001		| 0.04	| 0.14	| 0.19	| 0.23	| 0.0	|
| 0.1		| 0.04	| 0.06	| 0.06	| 0.02	| 0.0	|
| 10		| 0.0	| 0.0	| 0.0	| 0.0	| 0.0	|

## Average best IOU
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.068	| 0.117	| 0.329	| 0.396	| 0.057	|
| 1e-05		| 0.068	| 0.117	| 0.33	| 0.387	| 0.057	|
| 0.001		| 0.068	| 0.116	| 0.329	| 0.353	| 0.057	|
| 0.1		| 0.068	| 0.069	| 0.071	| 0.067	| 0.0	|
| 10		| 0.066	| 0.066	| 0.066	| 0.066	| 0.0	|

Conclusion:
No learning rate schedule better than decreasing per epoch.
Decreasing every 100 eprochs probably better.



###############################################################################
# Weighted loss + no learning rate schedule
###############################################################################
# Val results
lr 0.0001 curve doesn't decrease much
lr 0.01 seems to have nicer cruve shape (and also finishes in a lower loss)
lr=0.1 is more noisier (not a lot) but converges to low value, too
lambda of 0.1 and 10 cause no learning or divergence (too high)


## Average best val loss (less is better)
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0	    	| 0.562	| 0.449	| 0.302	| 0.245	| inf	|
| 1e-05		| 0.562	| 0.449	| 0.302	| 0.244	| inf	|
| 0.001		| 0.562	| 0.449	| 0.301	| 0.241	| inf	|
| 0.1		| 0.563	| 0.533	| 0.541	| 0.544	| inf	|
| 10		| 0.619	| 0.667	| 0.681	| 0.681	| inf	|

## Average best epoch
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		| 100.0	| 99.0	| 98.0	| 95.0	| 0.0	|
| 1e-05		| 100.0	| 99.0	| 98.0	| 96.0	| 0.0	|
| 0.001		| 100.0	| 99.0	| 98.0	| 96.0	| 0.0	|
| 0.1		| 100.0	| 74.0	| 8.0	| 28.0	| 0.0	|
| 10		| 1.0	| 1.0	| 10.0	| 63.0	| 0.0	|

## Average best threshold
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		| 0.25	| 0.34	| 0.52	| 0.68	| 0.0	|
| 1e-05		| 0.25	| 0.34	| 0.52	| 0.68	| 0.0	|
| 0.001		| 0.25	| 0.34	| 0.52	| 0.62	| 0.0	|
| 0.1		| 0.23	| 0.31	| 0.0	| 0.0	| 0.0	|
| 10		| 0.0	| 0.0	| 0.0	| 0.0	| 0.0	|

## Average best IOU
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0	    	| 0.071	| 0.192	| 0.374	| 0.451	| 0.016	|
| 1e-05		| 0.071	| 0.192	| 0.374	| 0.457	| 0.016	|
| 0.001		| 0.071	| 0.192	| 0.375	| 0.461	| 0.016	|
| 0.1		| 0.071	| 0.097	| 0.066	| 0.066	| 0.058	|
| 10		| 0.066	| 0.066	| 0.066	| 0.066	| 0.0	|


##############################################################################
# Weighted loss + learning rate schedule
##############################################################################
# Val Losses
Similar behavior as withhout learning rate schedule. Val losses seem to converge to smaller values

## Average best val loss (less is better)
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.564	| 0.462	| 0.349	| 0.269	| inf	|
| 1e-05		| 0.564	| 0.464	| 0.349	| 0.268	| inf	|
| 0.001		| 0.564	| 0.463	| 0.349	| 0.262	| inf	|
| 0.1		| 0.564	| 0.534	| 0.541	| 0.545	| inf	|
| 10		| 0.619	| 0.667	| 0.681	| 0.681	| inf	|

## Average best epoch
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 100.0	| 98.0	| 84.0	| 91.0	| 0.0	|
| 1e-05		| 100.0	| 97.0	| 88.0	| 93.0	| 0.0	|
| 0.001		| 100.0	| 97.0	| 86.0	| 83.0	| 0.0	|
| 0.1		| 100.0	| 84.0	| 8.0	| 9.0	| 0.0	|
| 10		| 1.0	| 1.0	| 1.0	| 10.0	| 0.0	|

## Average best threshold
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		    | 0.24	| 0.36	| 0.41	| 0.62	| 0.0	|
| 1e-05		| 0.24	| 0.36	| 0.41	| 0.62	| 0.0	|
| 0.001		| 0.24	| 0.36	| 0.41	| 0.64	| 0.0	|
| 0.1		| 0.23	| 0.31	| 0.28	| 0.0	| 0.0	|
| 10		| 0.0	| 0.0	| 0.0	| 0.0	| 0.0	|

## Average best IOU
| Lambda/LR	| 0.0001| 0.001	| 0.01	| 0.1	| 1 	|
| 0		| 0.071	| 0.177	| 0.311	| 0.43	| 0.016	|
| 1e-05		| 0.071	| 0.175	| 0.31	| 0.43	| 0.016	|
| 0.001		| 0.071	| 0.175	| 0.311	| 0.436	| 0.016	|
| 0.1		| 0.071	| 0.097	| 0.073	| 0.066	| 0.058	|
| 10		| 0.066	| 0.066	| 0.066	| 0.066	| 0.0	|
